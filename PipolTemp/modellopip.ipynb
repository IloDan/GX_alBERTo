{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, mask_embedding=False, w2v_init=\"uniform\", pad_to_0=False):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size,\n",
    "                                          output_dim=embed_dim,\n",
    "                                          embeddings_initializer=w2v_init,\n",
    "                                          #embeddings_constraint=tf.keras.constraints.UnitNorm(axis=0),\n",
    "                                          )\n",
    "        self.mask_embedding = mask_embedding\n",
    "        self.embed_dim      = embed_dim\n",
    "        self.pad_to_0       = pad_to_0\n",
    "    def call(self, x):\n",
    "        if self.mask_embedding is not False:\n",
    "            mask = tf.cast(x != self.mask_embedding, np.float32)\n",
    "        x = self.token_emb(x)\n",
    "        if self.mask_embedding is not False:\n",
    "            mask_embedded = tf.tile(tf.expand_dims(mask, -1), [1, 1, self.embed_dim])\n",
    "            if self.pad_to_0:\n",
    "                return x*mask_embedded, mask\n",
    "            else:\n",
    "                return x, mask\n",
    "        else:\n",
    "            return x, tf.zeros([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, p_enc=\"w2v\"):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.p_enc = p_enc\n",
    "\n",
    "    def call(self, inputs, masked=None, training=None):\n",
    "        if self.p_enc == \"relative\":\n",
    "            x, x_q, x_k, x_v = inputs\n",
    "            if masked is not None:\n",
    "                attn_output, att_scores = self.att(x_q, x_k, x_v, return_attention_scores=True, attention_mask=masked)\n",
    "            else:\n",
    "                attn_output, att_scores = self.att(x_q, x_k, x_v, return_attention_scores=True)\n",
    "            inputs = x\n",
    "        else:\n",
    "            if masked is not None:\n",
    "                attn_output, att_scores = self.att(inputs, inputs, inputs, return_attention_scores=True, attention_mask=masked)\n",
    "            else:\n",
    "                attn_output, att_scores = self.att(inputs, inputs, inputs, return_attention_scores=True)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output), att_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only new_embedding\n",
    "class PositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim, rate):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        maxlen = tf.shape(x)[-2]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        x += self.pos_emb(positions)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prepare_AttentionMask(layers.Layer):\n",
    "    def __init__(self, add_reg, pool_size, name=None):\n",
    "        super(prepare_AttentionMask, self).__init__(name = name)\n",
    "        self.add_reg = add_reg\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def call(self, x, training):\n",
    "        x = tf.ones(tf.shape(x)) - x\n",
    "        x = tf.expand_dims(x, -1)\n",
    "        x = layers.MaxPool1D(pool_size=self.pool_size, strides=None, padding=\"valid\")(x)\n",
    "        if self.add_reg:\n",
    "            x = tf.concat([tf.zeros((tf.shape(x)[0], 1, 1)), x], axis=1)\n",
    "        x = tf.ones(tf.shape(x)) - x\n",
    "        x = tf.matmul(x, x, transpose_b=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class prepare_AttentionMask(nn.Module):\n",
    "    def __init__(self, add_reg, pool_size):\n",
    "        super(prepare_AttentionMask, self).__init__()\n",
    "        self.add_reg = add_reg\n",
    "        self.pool_size = pool_size\n",
    "        self.maxpool = nn.MaxPool1d(pool_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = 1 - x\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = self.maxpool(x)\n",
    "        if self.add_reg:\n",
    "            zeros = torch.zeros(x.size(0), 1, 1).to(x.device)\n",
    "            x = torch.cat([zeros, x], dim=1)\n",
    "        x = 1 - x\n",
    "        x = torch.matmul(x, x.transpose(-1, -2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add_REG(layers.Layer):\n",
    "    def __init__(self, embed_dim, rate=0.01, name=None):\n",
    "        super(Add_REG, self).__init__(name = name)\n",
    "        self.reg_emb = layers.Embedding(input_dim=1, output_dim=embed_dim)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        REG     = tf.range(start=0, limit=1, delta=1)\n",
    "        reg_emb = self.reg_emb(REG)\n",
    "        reg_emb = self.dropout(reg_emb, training=training)\n",
    "        reg_emb = tf.tile(tf.expand_dims(reg_emb, 0), [tf.shape(x)[0], 1, 1])\n",
    "        concat  = tf.concat([reg_emb, x], 1)\n",
    "        return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Add_REG(nn.Module):\n",
    "    def __init__(self, embed_dim, rate=0.01):\n",
    "        super(Add_REG, self).__init__()\n",
    "        self.reg_emb = nn.Embedding(1, embed_dim)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        REG = torch.arange(0, 1).long()\n",
    "        reg_emb = self.reg_emb(REG)\n",
    "        reg_emb = self.dropout(reg_emb)\n",
    "        reg_emb = reg_emb.expand(x.size(0), -1, -1)\n",
    "        concat = torch.cat([reg_emb, x], dim=1)\n",
    "        return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_model(self):\n",
    "    embedding_layer = TokenEmbedding(self.maxlen, self.vocab_size, self.w2v_embdim, self.mask_embedding, self.w2v_init, self.pad_to_0)        \n",
    "    pooler = layers.Dense(self.embed_dim)\n",
    "    trans_layers = TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate, self.p_enc)\n",
    "    #inputs\n",
    "    inputs = []\n",
    "    input1 = layers.Input(shape=(self.maxlen), name=\"sequence\")\n",
    "    inputs.append(input1)\n",
    "    embedded, masked = embedding_layer(input1)\n",
    "\n",
    "    x1 = layers.Conv1D(filters=self.embed_dim, kernel_size=6, strides=1, padding=\"same\",\n",
    "    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
    "    x2 = layers.Conv1D(filters=self.embed_dim, kernel_size=9, strides=1, padding=\"same\",\n",
    "    dilation_rate=1, groups=1, activation=\"relu\", kernel_initializer='glorot_normal')(embedded)\n",
    "    x12 = layers.Concatenate()([x1,x2])\n",
    "    x12 = layers.Dense(self.embed_dim, activation=\"relu\")(x12)\n",
    "    #x12 = layers.Dropout(self.dropout_rate)(x12)\n",
    "    skip = layers.Add()([x12, embedded])\n",
    "    x = layers.AveragePooling1D(pool_size=self.pool_size, strides=None, padding=\"valid\")(skip)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    posemblen = x.shape[1]\n",
    "    posenc_layer = PositionEmbedding(posemblen, self.embed_dim, self.t_rate)\n",
    "    x = posenc_layer(x)\n",
    "    masked = prepare_AttentionMask(self.add_reg, self.pool_size)(masked)\n",
    "    if self.add_reg:\n",
    "        add_reg = Add_REG(self.embed_dim)\n",
    "        x = add_reg(x)\n",
    "    att_scores = []\n",
    "    x, atts = trans_layers[i](x, masked=masked)\n",
    "    att_scores.append(atts)\n",
    "    x = pooler(x[:, 0])\n",
    "    x = tf.keras.activations.tanh(x)\n",
    "    #dense1\n",
    "    x = layers.Dense(self.dense, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(self.dropout_rate)(x)\n",
    "    #dense2\n",
    "    x = layers.Dense(self.dense, activation=\"gelu\")(x)\n",
    "    x = layers.Dropout(self.dropout_rate)(x)\n",
    "    #output\n",
    "    output = layers.Dense(self.output_neurons, activation=\"linear\", name=\"Regression_Output\")(x)\n",
    "    if len(att_scores) > 0:\n",
    "        #att_scores = Forward(name=\"att_scores\")(att_scores)\n",
    "        self.model = tf.keras.Model(\n",
    "            inputs=inputs,\n",
    "            outputs={'Regression_Output': output, 'Attention_Scores': att_scores\n",
    "                    #  , 'M_atts': M_atts\n",
    "                        },\n",
    "            )\n",
    "    else:\n",
    "        self.model = tf.keras.Model(\n",
    "            inputs=inputs,\n",
    "            outputs={'Regression_Output': output},\n",
    "            )\n",
    "    self.model.summary()\n",
    "    img = tf.keras.utils.plot_model(self.model, f\"{self.model_type}.png\", show_shapes=True)\n",
    "    display(img)\n",
    "    print(f\"\\nParameters:\\n\")\n",
    "    for k, v in vars(self).items():\n",
    "        pad = ' '.join(['' for _ in range(25-len(k))])\n",
    "        print(k, f\" :{pad}\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding_layer = TokenEmbedding(self.maxlen, self.vocab_size, self.w2v_embdim, self.mask_embedding, self.w2v_init, self.pad_to_0)\n",
    "        self.pooler = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.trans_layers = TransformerBlock(self.embed_dim, self.num_heads, self.ff_dim, self.t_rate, self.p_enc)\n",
    "        self.conv1 = nn.Conv1d(self.embed_dim, self.embed_dim, kernel_size=6, stride=1, padding=\"same\")\n",
    "        self.conv2 = nn.Conv1d(self.embed_dim, self.embed_dim, kernel_size=9, stride=1, padding=\"same\")\n",
    "        self.fc = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.avgpool = nn.AvgPool1d(self.pool_size)\n",
    "        self.batchnorm = nn.BatchNorm1d(self.embed_dim)\n",
    "        self.posenc_layer = PositionEmbedding(posemblen, self.embed_dim, self.t_rate)\n",
    "        self.add_reg = Add_REG(self.embed_dim)\n",
    "        self.dense1 = nn.Linear(self.dense, self.dense)\n",
    "        self.dropout1 = nn.Dropout(self.dropout_rate)\n",
    "        self.dense2 = nn.Linear(self.dense, self.dense)\n",
    "        self.dropout2 = nn.Dropout(self.dropout_rate)\n",
    "        self.output = nn.Linear(self.dense, self.output_neurons)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded, masked = self.embedding_layer(x)\n",
    "        x1 = F.relu(self.conv1(embedded))\n",
    "        x2 = F.relu(self.conv2(embedded))\n",
    "        x12 = torch.cat((x1, x2), dim=1)\n",
    "        x12 = F.relu(self.fc(x12))\n",
    "        skip = x12 + embedded\n",
    "        x = self.avgpool(skip)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.posenc_layer(x)\n",
    "        masked = prepare_AttentionMask(self.add_reg, self.pool_size)(masked)\n",
    "        if self.add_reg:\n",
    "            x = self.add_reg(x)\n",
    "        x, atts = self.trans_layers(x, masked=masked)\n",
    "        x = self.pooler(x[:, 0])\n",
    "        x = torch.tanh(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.gelu(self.dense2(x))\n",
    "        x = self.dropout2(x)\n",
    "        output = self.output(x)\n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
